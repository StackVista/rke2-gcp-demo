version: '3'

dotenv: ['gcp.tfvars']

vars:
  SSH_CMD: ssh -o "UserKnownHostsFile=/dev/null" -oStrictHostKeyChecking=no
  B64_DECODE: base64 -D
  YQ_UCKD: yq '.users[] | select(.name=="default") | .user.client-key-data'
  YQ_UCCD: yq '.users[] | select(.name=="default") | .user.client-certificate-data'
  YQ_CA: yq '.clusters[] | select(.name=="default") | .cluster.certificate-authority-data'
  YQ_CSERVER: yq '.clusters[] | select(.name=="default") | .cluster.server'
  DIR: ./rancher-gcp

tasks:
  terraform-init:
    cmds:
      - "terraform -chdir={{.DIR}} init -upgrade"
  terraform-output:
    cmds:
      - "terraform -chdir={{.DIR}} refresh -var-file=../gcp.tfvars"
      - "terraform -chdir={{.DIR}} output -json > ./.provisioned_env.json"
  terraform-plan:
    cmds:
      - "terraform -chdir={{.DIR}} plan -var-file=../gcp.tfvars"
  terraform-apply:
    cmds:
      - "terraform -chdir={{.DIR}} apply -var-file=../gcp.tfvars"
  terraform-destroy:
    cmds:
      - "terraform -chdir={{.DIR}} destroy -var-file=../gcp.tfvars"
  kubeconfig:
    cmds:
      - kubectl config set-credentials {{.CLUSTER}}-user --client-certificate=<(cat {{.DIR}}/{{.CLUSTER}}_kubeconfig.yaml | {{.YQ_UCCD}} | {{.B64_DECODE}}) --client-key=<(cat {{.DIR}}/{{.CLUSTER}}_kubeconfig.yaml | {{.YQ_UCKD}} | {{.B64_DECODE}}) --embed-certs=true
      - kubectl config set-cluster {{.CLUSTER}} --server=$(cat {{.DIR}}/{{.CLUSTER}}_kubeconfig.yaml | {{.YQ_CSERVER}}) --certificate-authority=<(cat {{.DIR}}/{{.CLUSTER}}_kubeconfig.yaml | {{.YQ_CA}} | {{.B64_DECODE}}) --embed-certs=true
      - kubectl config set-context {{.CLUSTER}} --cluster={{.CLUSTER}} --user={{.CLUSTER}}-user
  all-kubeconfig:
    vars:
      CLUSTERS:
        sh: "cat .provisioned_env.json | jq -r '.\"rke2-clusters\".value[].cluster_name, .\"rancher-cluster\".value.cluster_name, .\"otel-cluster\".value.cluster_name' | tr '\n' ' '"
    cmds:
      - for: { var: CLUSTERS }
        task: kubeconfig
        vars: { CLUSTER: '{{.ITEM}}' }
  check-cluster-connection:
      - kubectl --context {{.CLUSTER}} get nodes
  helm-repo-add:
    internal: true
    cmds:
      - helm repo add --force-update stackstate-internal https://helm-internal.stackstate.io/
      - helm repo add --force-update deliveryhero https://charts.deliveryhero.io/
      - helm repo update
  deploy-sock-shop:
    deps:
      - helm-repo-add
    vars:
      CLUSTER_0:
        sh: "cat .provisioned_env.json | jq -r '.\"rke2-clusters\".value[0].cluster_name'"
      PUBLIC_IP_1:
        sh: "cat .provisioned_env.json | jq -r '.\"rke2-clusters\".value[1].worker_loadbalancer_ip'"
      CLUSTER_1:
        sh: "cat .provisioned_env.json | jq -r '.\"rke2-clusters\".value[1].cluster_name'"
    cmds:
      - helm upgrade --install --kubeconfig {{.DIR}}/{{.CLUSTER_1}}_kubeconfig.yaml --namespace sock-shop --create-namespace --values sock-shop/values-fulfillment.yaml --set-string 'payment.ingress.hostname'='payment.{{.PUBLIC_IP_1}}.sslip.io'  --set-string 'shipping.ingress.hostname'='shipping.{{.PUBLIC_IP_1}}.sslip.io' sock-shop stackstate-internal/sock-shop
      - helm upgrade --install --kubeconfig {{.DIR}}/{{.CLUSTER_0}}_kubeconfig.yaml --namespace sock-shop --create-namespace --values sock-shop/values-shop.yaml --set-string 'orders.components.paymentHost'='payment.{{.PUBLIC_IP_1}}.sslip.io'  --set-string 'orders.components.shippingHost'='shipping.{{.PUBLIC_IP_1}}.sslip.io' sock-shop stackstate-internal/sock-shop
      - task deploy-locust
  deploy-locust:
    vars:
      CLUSTER_0:
        sh: "cat .provisioned_env.json | jq -r '.\"rke2-clusters\".value[0].cluster_name'"
    cmds:
      - kubectl apply --kubeconfig {{.DIR}}/{{.CLUSTER_0}}_kubeconfig.yaml -f sock-shop/locust-config.yaml
      - helm upgrade --kubeconfig {{.DIR}}/{{.CLUSTER_0}}_kubeconfig.yaml --install -n sock-shop --values sock-shop/locust-values.yaml --version 0.31.1 locust deliveryhero/locust
